生成时的温度（`temperature`）参数。在文本生成任务中，`temperature` 是一个控制生成文本时的随机性和多样性的超参数。它主要影响模型在选择下一个词时对概率分布的处理方式。具体来说，`temperature` 用于对模型输出的概率分布进行缩放，从而调整生成的随机性。

### 1. **工作原理**
- **高温度（`temperature > 1`）**：
    - 当 `temperature` 值大于 1 时，模型会放大输出概率分布中的所有可能性。这意味着模型会更倾向于探索生成中不太常见的词汇，从而增加生成的多样性和随机性，但同时也可能导致生成的文本质量下降，变得不太连贯或有些意外的词汇出现。

- **低温度（`0 < temperature < 1`）**：
    - 当 `temperature` 值介于 0 和 1 之间时，模型会使概率分布更尖锐，即模型倾向于选择概率更高的词。这会减少生成的随机性，使输出更具确定性和连贯性，但也可能导致生成的文本较为单调，缺乏多样性。

- **`temperature = 1`**：
    - 当 `temperature` 等于 1 时，模型按照原始的概率分布进行词汇选择，没有额外的缩放影响。这是通常的默认设置，意味着模型生成文本的随机性与其训练时的行为一致。

### 2. **公式表示**
生成时的概率分布会根据温度参数 `t` 进行缩放：
\[
P'(x_i) = \frac{\exp(\log(P(x_i)) / t)}{\sum_j \exp(\log(P(x_j)) / t)}
\]
其中：
- \( P(x_i) \) 是模型生成的初始概率。
- \( t \) 是温度参数。
- \( P'(x_i) \) 是调整后的概率。

### 3. **应用场景**
- **高创意生成**：在对话生成、故事生成或需要创造性内容的任务中，较高的温度（如 1.5 或 2）可以帮助模型生成更具多样性和创意的内容。
- **任务要求确定性**：在需要高准确性和确定性的任务中，如新闻摘要或技术文档生成，较低的温度（如 0.7 或 0.5）可以帮助模型生成更准确、连贯的输出。

### 4. **总结**
- **温度** 控制生成时的随机性和多样性。高温度增加随机性和多样性，但可能降低文本连贯性；低温度则增加生成的确定性和连贯性，但可能导致文本过于单调。
- 根据具体的生成任务和目标，可以调整 `temperature` 以平衡生成内容的多样性和质量。

Top-p（也称为核采样）和Top-k，它们是生成文本时常用的策略，用于控制生成过程中选择词汇的随机性。

### 1. **Top-k Sampling**
- **概念**：Top-k采样是一种策略，它限制模型只从概率最高的 `k` 个词汇中进行采样，而忽略其余的词汇。这可以避免模型生成那些概率非常低的词汇，从而减少生成的噪声。

- **工作原理**：
    - 模型首先计算每个词的概率分布。
    - 然后，选择概率最高的 `k` 个词，并重新归一化这些词的概率，使它们的总和为 1。
    - 最终，从这些概率重新归一化的 `k` 个词汇中进行随机采样。

- **影响**：
    - **低 k 值**：当 `k` 值较低时，生成的文本会更加确定性，可能比较保守或重复。
    - **高 k 值**：较高的 `k` 值允许更多的词被选择，生成的文本可能更具多样性，但也可能引入更多不连贯的内容。

### 2. **Top-p Sampling (Nucleus Sampling)**
- **概念**：Top-p 采样是另一种策略，它不像Top-k那样选择固定数量的 `k` 个词，而是动态地选择一组累积概率超过 `p` 的词汇，这组词汇的累积概率刚好达到或超过 `p`（如 0.9）。这样做是为了在保证生成质量的同时，灵活地控制生成的多样性。

- **工作原理**：
    - 模型计算出所有词的概率分布，并按概率从高到低排序。
    - 然后，从排序后最高的词开始累积概率，直到累积概率超过设定的阈值 `p`（通常在 0.8 到 0.95 之间）。
    - 最终，从这组累计概率超过 `p` 的词汇中进行随机采样。

- **连乘概率还是连加概率？**
    - **Top-p 计算的是累积概率（连加概率）**：具体来说，Top-p 采样是将排序后的词的概率进行累加，直到总和达到 `p`，再从这些词汇中进行采样。它不涉及连乘概率的计算。

- **影响**：
    - **低 p 值**：模型会更加保守，生成的文本会更有连贯性，但可能会显得单调。
    - **高 p 值**：生成的文本会更具多样性，但也可能引入不连贯的内容。

### 3. **总结**
- **Top-k**：限制模型只从概率最高的 `k` 个词汇中进行采样。`k` 值越小，生成的文本越确定；`k` 值越大，生成的多样性越高。
- **Top-p**：动态选择累积概率超过 `p` 的词汇，并从中采样。`p` 越低，生成的文本越确定；`p` 越高，生成的多样性越高。Top-p 计算的是累积概率（连加概率），而不是连乘概率或单独计算每个token的概率。

通过调整 `Top-k` 和 `Top-p` 参数，可以平衡生成文本的连贯性和多样性，以满足不同任务的需求。

## model.chat和mode.generate生成上有什么区别
chat 能够记住上下文 是多轮对话的格式。并且在使用chat(instruct)模型的时候会apply_template
generate 底层的接口 单轮并且需要自己添加模板。

Role Prompt、CoT（Chain of Thought）、PoT（Program of Thought）等概念是近年来在自然语言处理（NLP）领域提出的一些策略或方法，旨在提高语言模型的性能、解释性或推理能力。以下是对这些概念的详细介绍：

### 1. **Role Prompt**
- **概念**：Role Prompt 是一种在对话或任务执行中赋予模型特定“角色”的提示方式。通过明确模型的角色或身份，可以引导模型以特定的方式回答问题或生成文本。例如，让模型扮演“老师”时，它的回答可能会更倾向于解释性和教育性。
- **工作原理**：
  - 在输入提示中加入描述角色的文本，使得模型在生成输出时能够考虑到这个角色的属性和行为方式。
  - 通过明确角色指示，模型可以在特定上下文中提供更适合的回答或内容。
- **应用场景**：
  - **对话系统**：在聊天机器人中赋予不同角色，以适应用户需求，如“医生”回答医疗问题、“客服”回答技术支持问题。
  - **任务引导**：在生成或推理任务中，通过角色设定引导模型以特定方式执行任务。

### 2. **CoT（Chain of Thought）说白了就是Step by step**
- **概念**：Chain of Thought 是一种推理策略，它通过让模型逐步解释或展示其思考过程，从而生成最终答案。CoT 旨在提高模型的推理能力，特别是在复杂问题或多步推理任务中。
- **工作原理**：
  - **逐步推理**：模型在生成答案时，首先通过多个中间步骤进行推理，每一步都解释或展示其思考的过程。最终输出是基于这些中间步骤的结果。
  - **逻辑链**：通过显式地展示推理链条，模型的输出更加透明，也有助于提高模型的解释性和准确性。
- **应用场景**：
  - **数学推理**：在解决复杂数学问题时，模型逐步推导每一步的结果。
  - **逻辑推理**：在需要多步推理的场景，如逻辑问题、谜题解决、复杂的决策任务等。

### 3. **PoT（Program of Thought）输出里包含代码**
- **概念**：Program of Thought 是一种推理范式，模型通过生成类代码或结构化步骤的方式来解决问题。与 CoT 类似，PoT 强调结构化推理，但它的重点在于将推理过程表示为类似程序的形式，便于模型进行复杂的、多步的逻辑推理。
- **工作原理**：
  - **代码生成**：模型通过生成类似编程语言的指令或伪代码，逐步执行逻辑推理。每一行代码代表推理的一个步骤，最终通过这些步骤得出答案。
  - **结构化思维**：PoT 借鉴了编程中的控制流概念，使得模型在处理复杂任务时能够有效地进行步骤分解和结果组合。
- **应用场景**：
  - **算法推理**：在需要执行特定算法的场景中，PoT 可以让模型以编程的方式逐步生成和执行每个算法步骤。
  - **复杂任务解决**：适用于需要多步、精确控制的推理任务，如数独解题、编程任务中的代码生成与解释。

### 4. **结合与应用**
- **Role Prompt + CoT/PoT**：通过将 Role Prompt 与 CoT 或 PoT 结合，模型可以在特定角色下进行更清晰的推理。例如，扮演“数学教师”角色的模型可以通过 CoT 逐步解释复杂的数学题解。
- **任务复杂度**：在处理复杂推理任务时，CoT 和 PoT 可以帮助模型逐步解决问题，减少错误率，并且让生成的过程更具解释性。
- **多样性与适应性**：Role Prompt 提供了在不同场景中灵活适应的能力，CoT 和 PoT 则增强了模型的推理和问题解决能力。结合使用这些方法，可以显著提高模型在复杂任务中的表现。

### **总结**
- **Role Prompt**：赋予模型特定角色，影响其输出的风格和内容。
- **CoT（Chain of Thought）**：通过逐步推理，展示模型的思考过程，提高复杂任务的推理能力和解释性。
- **PoT（Program of Thought）**：通过生成类似代码的结构化步骤，帮助模型执行复杂、多步的逻辑推理。

这些技术可以单独使用，也可以结合使用，以提高模型在特定任务中的性能、透明度和多样性。

BPE（Byte-Pair Encoding）和 WordPiece 是两种常用的分词算法，它们在处理文本时将词汇拆分为子词（subword）单元。这两种方法的目标都是在词汇表大小和表示能力之间取得平衡，特别是在处理未见词（OOV, Out-of-Vocabulary）时。尽管它们有相似之处，但在实现细节和构建子词单元的策略上有一些区别。

### 1. **BPE（Byte-Pair Encoding）**
- **概念**：BPE 是一种基于频率的子词分词算法。它通过迭代地合并出现频率最高的字符对（或子词对），逐步构建出子词单元。这种方法最早用于数据压缩，后来被应用于自然语言处理任务中。

- **工作原理**：
  1. **初始化**：从字符级别的分词开始，将每个单词拆分为单个字符（如 `hello` -> `h`, `e`, `l`, `l`, `o`）。
  2. **迭代合并**：找到出现频率最高的字符对，将其合并为一个新的子词单元。这个过程不断重复，直到达到预设的词汇表大小或不再有高频对可合并。
  3. **生成子词单元**：通过不断合并字符对，最终生成的词汇表中包含各种子词单元（如 `he`, `ll`, `o`）。

- **优点**：
  - **简单高效**：BPE通过频率统计的方式，能够快速生成子词单元。
  - **适应性强**：它能很好地处理词汇中的常见前缀、后缀和词干变化。
  - **可控词汇表大小**：用户可以控制合并的次数，从而生成不同大小的词汇表。

- **应用**：BPE广泛应用于许多神经网络模型中，如 OpenAI 的 GPT 系列。

### 2. **WordPiece**
- **概念**：WordPiece 是另一种基于统计的子词分词方法，最早由 Google 提出并应用于机器翻译任务中，后来被用于 BERT 等模型中。与 BPE 类似，WordPiece 也基于频率统计，但它更关注于通过概率最大化来选择最优的子词单元。

- **工作原理**：
  1. **初始化**：与 BPE 类似，WordPiece 也从字符级别开始，将每个单词拆分为单个字符。
  2. **基于概率的合并**：WordPiece 通过选择能最大化训练数据似然的子词单元来进行合并。它不是仅仅基于频率，而是通过计算子词序列的概率（通常使用联合概率），来决定哪些子词单元应该合并。
  3. **词汇表生成**：通过这种概率最大化的方式，逐步生成词汇表中的子词单元。

- **优点**：
  - **更加精细**：WordPiece 的合并策略考虑了上下文的概率，生成的子词单元通常能更好地适应具体任务的语言特征。
  - **更好的语言适应性**：在多语言模型中，WordPiece 能更有效地处理不同语言的复杂性。

- **应用**：WordPiece 被广泛应用于 BERT 及其变体模型中。

### 3. **主要区别**
- **合并策略**：
  - **BPE**：基于字符对的频率进行合并，简单且高效，但可能在某些情况下合并出不理想的子词单元。
  - **WordPiece**：基于最大化训练数据似然的策略进行合并，更精细，但计算相对复杂。

- **词汇表生成**：
  - **BPE**：可以通过控制合并次数来控制词汇表大小，灵活性较高。
  - **WordPiece**：词汇表大小由最大化概率的策略控制，可能更适应特定任务或语言数据。

- **处理未见词（OOV）**：
  - **BPE** 和 **WordPiece** 都擅长处理未见词，将其分解为子词单元，但 WordPiece 由于其概率最大化的策略，通常在生成更适合上下文的子词时表现更好。

### 4. **总结**
- **BPE**：通过频率统计的方式合并子词单元，简单高效，适合许多通用任务。
- **WordPiece**：通过概率最大化策略生成子词单元，更加精细，特别适合需要高语言适应性的任务，如多语言模型。

两者各有优劣，具体选择取决于任务的需求和模型的设计目标。

**KV Cache**（Key-Value Cache）是指在Transformer模型的自注意力机制中缓存已计算的注意力键（Key）和值（Value），以优化推理效率的一种技术。KV Cache的引入主要是在推理过程中，尤其是生成式任务中，提高生成速度并减少重复计算。

### **KV Cache的工作原理**

在Transformer的自注意力机制中，每一层都会计算输入的查询（Query）、键（Key）、和值（Value），并基于这些进行注意力计算。通常，在生成每个新词时，模型需要重新计算所有先前词汇的Query、Key、Value，并通过这些进行注意力计算。这种重复计算在长序列生成时变得非常昂贵。

**KV Cache**通过在生成序列的过程中缓存已经计算好的Key和Value，避免了在生成新词时重复计算这些值。具体过程如下：

1. **初始计算**：
  - 在生成序列的第一个词时，模型会正常计算Query、Key和Value，并将Key和Value缓存起来。

2. **后续生成**：
  - 在生成后续词时，模型只需要计算当前词的Query，而不需要重新计算前面所有词的Key和Value。
  - 在注意力计算中，模型直接使用缓存的Key和Value进行注意力权重计算。

### **KV Cache的优点**

1. **提高推理效率**：
  - **减少重复计算**：通过缓存Key和Value，模型在生成新词时无需重复计算前面词的Key和Value，大大减少了计算量。
  - **加速长序列生成**：在处理长序列生成任务（如对话生成、文本续写）时，KV Cache能够显著提升推理速度，尤其是在生成较长文本时，效率提升更加明显。

2. **降低计算成本**：
  - **减少GPU/TPU负载**：在实际应用中，缓存机制可以有效减少计算资源的使用，降低模型在推理过程中的GPU/TPU负载，适合部署在资源受限的环境中。
  - **节省内存**：尽管需要存储Key和Value，但相比每次重新计算整个序列的开销，KV Cache带来的整体内存和计算节省是明显的。

3. **保持生成质量**：
  - **无精度损失**：KV Cache不会影响生成文本的质量，因为它只是优化了计算过程，生成结果与不使用缓存时完全一致。

4. **应用广泛**：
  - **生成式模型**：KV Cache尤其适用于生成式模型，如GPT系列、对话模型等，在这些模型中，通过缓存机制可以显著提升响应速度。
  - **长文本处理**：在需要处理超长文本或进行长文本生成的任务中，KV Cache能够有效缓解计算压力，并提供更好的用户体验。

## 为什么缓存KV而不缓存Q
可以参考[知乎](https://zhuanlan.zhihu.com/p/622212228)
总结来说就是 Q 只在当前 token 的推断计算中有用到 而KV在后续所有 token 的推断计算中都得用
